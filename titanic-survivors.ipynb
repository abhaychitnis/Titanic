{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\abhay\\environments\\mludemy\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#%%writefile titanic-pre.py\n",
    "\n",
    "# %load titanic-pre.py\n",
    "# Data Preprocessing Template\n",
    "\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# common parameters\n",
    "\n",
    "missing_values_not_applicable = 0\n",
    "missing_values_drop_rows = 1\n",
    "missing_values_fill_mean = 2\n",
    "missing_values_drop_column = 3\n",
    "missing_values_not_decided = 4\n",
    "\n",
    "# Importing the dataset\n",
    "#dataset_complete = pd.read_csv('train.csv')\n",
    "preprocessing_override = pd.read_csv('preprocessing_override.csv')\n",
    "\n",
    "dataset_X = pd.read_csv('train.csv')\n",
    "dataset_y = dataset_X['Survived']\n",
    "\n",
    "dataset_X_verify = pd.read_csv('test.csv')\n",
    "\n",
    "del dataset_X['Survived']\n",
    "del preprocessing_override['Survived']\n",
    "\n",
    "def preprocess_ind (dataset, override):\n",
    "\n",
    "    unique_identification_cutoff = 0.01\n",
    "    drop_column_cutoff = 0.75\n",
    "\n",
    "    override_ind = 0\n",
    "    encoding_override_ind = 1\n",
    "    dropcolumn_override_ind = 2\n",
    "    \n",
    "    num_rows, num_columns = dataset.shape\n",
    "    null_list = dataset.isnull().sum()\n",
    "    encoding_override = list((override == encoding_override_ind).values)[0]\n",
    "    drop_column_override = list((override == dropcolumn_override_ind).values)[0]\n",
    "\n",
    "    return_list = []\n",
    "    category_encoding = []\n",
    "    missing_value_strategy = []\n",
    "    drop_column_strategy = []\n",
    "    normalize_strategy = []\n",
    "\n",
    "#Inspect every columns\n",
    "    for i in range(num_columns):\n",
    "# Inspect uniquenes \n",
    "\n",
    "#   Make sure that the null values are not considered while finding out the count of unique values for the column\n",
    "        presence_of_missing_values = False\n",
    "        percentage_missing_values = 100*null_list[i]/num_rows\n",
    "        column_with_notnull_values = dataset[dataset.columns[i]][dataset[dataset.columns[i]].notnull()==True]\n",
    "        if null_list[i] != 0:\n",
    "            presence_of_missing_values = True\n",
    "    \n",
    "# disparate_data_index is the ratio of number of unique values in the column to the number of rows. Lower values\n",
    "# indicates potential of uniqueness. A value of 1 indicates that every value in the coulmn is different than the rest\n",
    "        disparate_data_index = ((len(column_with_notnull_values.unique())) /(num_rows-null_list[i]))\n",
    "\n",
    "# Determine if the column is a candidate for feature encoding\n",
    "        if disparate_data_index > unique_identification_cutoff: \n",
    "            category_encoding.append(False)\n",
    "        elif encoding_override[i]:\n",
    "            category_encoding.append(False)\n",
    "        else:\n",
    "            category_encoding.append(True)\n",
    "# Inspect the data type\n",
    "        number_datatype = False\n",
    "        if dataset[dataset.columns[i]].dtype in ('int64', 'float64'):\n",
    "            number_datatype = True\n",
    "    \n",
    "        if presence_of_missing_values:\n",
    "            if percentage_missing_values > 50:\n",
    "#Set the missing value strategy to removing the column(or feature)\n",
    "                missing_value_strategy.append(3)\n",
    "            elif percentage_missing_values < 5:\n",
    "#Set the missing value strategy to removing the rows with missing value\n",
    "                missing_value_strategy.append(1)\n",
    "            elif number_datatype:\n",
    "#Set the missing value strategy to setting the value to mean of the column values\n",
    "                missing_value_strategy.append(2)\n",
    "            else:\n",
    "#Set the missing value strategy to UNKNOWN. This is related to non-numeric fields\n",
    "                missing_value_strategy.append(4)\n",
    "        else:\n",
    "#Set the missing value strategy to not applicable as there are no missing values\n",
    "            missing_value_strategy.append(0)\n",
    "    \n",
    "        if disparate_data_index < drop_column_cutoff:\n",
    "            drop_column_strategy.append(False)\n",
    "        elif drop_column_override[i]:\n",
    "            drop_column_strategy.append(False)\n",
    "        else:\n",
    "            drop_column_strategy.append(True)\n",
    "        \n",
    "        \n",
    "        if category_encoding[i]:\n",
    "            normalize_strategy.append(False)\n",
    "        elif missing_value_strategy == 3:\n",
    "            normalize_strategy.append(False)\n",
    "        elif drop_column_strategy[i]:\n",
    "            normalize_strategy.append(False)\n",
    "        elif number_datatype:\n",
    "            normalize_strategy.append(True)\n",
    "        else:\n",
    "            normalize_strategy.append(False)\n",
    "            \n",
    "    return_list.append(category_encoding)\n",
    "    return_list.append(missing_value_strategy)\n",
    "    return_list.append(drop_column_strategy)\n",
    "    return_list.append(normalize_strategy)\n",
    "    \n",
    "    return (return_list)\n",
    "    \n",
    "preprocess_list = preprocess_ind(dataset_X, preprocessing_override)\n",
    "\n",
    "category_encoding_ind = preprocess_list[0]\n",
    "missing_values_strategy = preprocess_list[1]\n",
    "drop_column_strategy = preprocess_list[2]\n",
    "normalize_strategy = preprocess_list[3]\n",
    "\n",
    "\n",
    "y = dataset_y.iloc[:].values\n",
    "X = dataset_X.iloc[:, :].values\n",
    "X_verify = dataset_X_verify.iloc[:, :].values\n",
    "\n",
    "\n",
    "\n",
    "# Taking care of missing data by dropping the rows\n",
    "\n",
    "missing_columns_drop_rows = list(np.where(np.array(missing_values_strategy) == missing_values_drop_rows))[0]\n",
    "\n",
    "for i in missing_columns_drop_rows:\n",
    "    indices_of_empty_rows = np.where((pd.isnull(X[:,i]) == True))[0]\n",
    "    \n",
    "    X = np.delete(X, indices_of_empty_rows , axis=0)\n",
    "    y = np.delete(y, indices_of_empty_rows , axis=0)\n",
    "\n",
    "    indices_of_empty_rows_test = np.where((pd.isnull(X_test[:,i]) == True))[0]\n",
    "    X_verify = np.delete(X_verify, indices_of_empty_rows_test , axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# Taking care of missing data by filling with mean values\n",
    "\n",
    "missing_columns_tobe_filled_with_mean = list(np.where(np.array(missing_values_strategy) == missing_values_fill_mean))[0]\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "\n",
    "for i in missing_columns_tobe_filled_with_mean:\n",
    "    imputer = imputer.fit(X[:, i:i+1])\n",
    "    X[:, i:i+1] = imputer.transform(X[:, i:i+1])\n",
    "    \n",
    "    imputer = imputer.fit(X_test[:, i:i+1])\n",
    "    X_verify[:, i:i+1] = imputer.transform(X_verify[:, i:i+1])\n",
    "\n",
    "\n",
    "# Taking care of missing data by dropping columns\n",
    "\n",
    "missing_columns_drop_column = list(np.where(np.array(missing_values_strategy) == missing_values_drop_column))[0]\n",
    "\n",
    "# Delete columns from X matching the index numbers in missing_columns_drop_column\n",
    "\n",
    "X = np.delete(X, missing_columns_drop_column , axis=1)\n",
    "X_verify = np.delete(X_verify, missing_columns_drop_column , axis=1)\n",
    "\n",
    "\n",
    "# Delete items from missing_values_strategy corresponding to the columns dropped in X\n",
    "\n",
    "missing_values_strategy = list(np.delete(np.array(missing_values_strategy), missing_columns_drop_column , axis=0))\n",
    "\n",
    "# Delete items from category_encoding_ind corresponding to the columns dropped in X\n",
    "\n",
    "category_encoding_ind = list(np.delete(np.array(category_encoding_ind), missing_columns_drop_column , axis=0))\n",
    "\n",
    "# Delete items from drop_column_strategy corresponding to the columns dropped in X\n",
    "\n",
    "drop_column_strategy = list(np.delete(np.array(drop_column_strategy), missing_columns_drop_column , axis=0))\n",
    "\n",
    "# Delete items from normalize_strategy corresponding to the columns dropped in X\n",
    "normalize_strategy = list(np.delete(np.array(normalize_strategy), missing_columns_drop_column , axis=0))\n",
    "\n",
    "\n",
    "\n",
    "# Drop columns corresponding to the columns marked in preprocessing as they are not likely to be relevant\n",
    "\n",
    "drop_column = list(np.where(np.array(drop_column_strategy) == True))[0]\n",
    "\n",
    "#print (drop_column)\n",
    "\n",
    "# Delete columns from X matching the index numbers in missing_columns_drop_column\n",
    "\n",
    "X = np.delete(X, drop_column , axis=1)\n",
    "X_verify = np.delete(X_verify, drop_column , axis=1)\n",
    "\n",
    "# Delete items from missing_values_strategy corresponding to the columns dropped in X\n",
    "\n",
    "missing_values_strategy = list(np.delete(np.array(missing_values_strategy), drop_column , axis=0))\n",
    "\n",
    "# Delete items from category_encoding_ind corresponding to the columns dropped in X\n",
    "\n",
    "category_encoding_ind = list(np.delete(np.array(category_encoding_ind), drop_column , axis=0))\n",
    "\n",
    "# Delete items from normalize_strategy corresponding to the columns dropped in X\n",
    "\n",
    "normalize_strategy = list(np.delete(np.array(normalize_strategy), drop_column , axis=0))\n",
    "\n",
    "# Delete items from drop_column_strategy corresponding to the columns dropped in X\n",
    "\n",
    "drop_column_strategy = list(np.delete(np.array(drop_column_strategy), missing_columns_drop_column , axis=0))\n",
    "\n",
    "if not drop_column_strategy:\n",
    "    print (\"something wrong !\")\n",
    "\n",
    "# For unique string column that can be cosidered as classification, convert into classification encoding using onecode\n",
    "\n",
    "category_encoding_columns = list(np.where(np.array(category_encoding_ind) == True))[0]\n",
    "\n",
    "if (category_encoding_columns.any()):\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    labelencoder_X = LabelEncoder()\n",
    "\n",
    "    for i in (category_encoding_columns):\n",
    "        X[:, i] = labelencoder_X.fit_transform(X[:, i])\n",
    "        \n",
    "        X_verify_drop_rows = np.where((pd.isnull(X_verify[:,i]) == True))[0]\n",
    "        if (X_verify_drop_rows.any()):\n",
    "            X_verify = np.delete(X_verify, X_verify_drop_rows , axis=0)\n",
    "        X_verify[:,i] = labelencoder_X.transform(X_verify[:, i])\n",
    "\n",
    "    X_extract = X[:,category_encoding_columns]\n",
    "    X_verify_extract = X_verify[:,category_encoding_columns]\n",
    "    onehotencoder = OneHotEncoder(categorical_features = 'all')    \n",
    "    X_extract_encoded = onehotencoder.fit_transform(X_extract).toarray()\n",
    "    X_verify_extract_encoded = onehotencoder.fit_transform(X_verify_extract).toarray()\n",
    "\n",
    "    X = np.delete(X, category_encoding_columns , axis=1)\n",
    "    X_verify = np.delete(X_verify, category_encoding_columns , axis=1)\n",
    "    category_encoding_ind = list(np.delete(np.array(category_encoding_ind), category_encoding_columns , axis=0))\n",
    "# Delete items from normalize_strategy corresponding to the columns dropped in X\n",
    "    normalize_strategy = list(np.delete(np.array(normalize_strategy), category_encoding_columns , axis=0))\n",
    "\n",
    "    \n",
    "    X = np.c_[X, X_extract_encoded]\n",
    "    X_verify = np.c_[X_verify, X_verify_extract_encoded]\n",
    "\n",
    "# For numeric columns, scale the values appropriately\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "normalize_strategy_columns = list(np.where(np.array(normalize_strategy) == True))[0]\n",
    "for i in normalize_strategy_columns:\n",
    "\n",
    "    X[:, [i]] = sc_X.fit_transform(X[:, i].reshape(-1, 1))\n",
    "    X_verify_drop_rows = np.where((pd.isnull(X_verify[:,i]) == True))[0]\n",
    "    if (X_verify_drop_rows.any()):\n",
    "        X_verify = np.delete(X_verify, X_verify_drop_rows , axis=0)\n",
    "\n",
    "    X_verify[:,[i]] = sc_X.transform(X_verify[:,i].reshape(-1,1))\n",
    "\n",
    "#sc_y = StandardScaler()\n",
    "#y_train = sc_y.fit_transform(y_train)\n",
    "print (\"All OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_X_verify.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[110  22]\n",
      " [ 31  60]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.83      0.81       132\n",
      "          1       0.73      0.66      0.69        91\n",
      "\n",
      "avg / total       0.76      0.76      0.76       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print (cm)\n",
    "print (cr)\n",
    "#for i in range(len(y_pred)):\n",
    "#    print (\"True y value : \", y_test[i],\"Predicted y values : \", y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[111  21]\n",
      " [ 33  58]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.84      0.80       132\n",
      "          1       0.73      0.64      0.68        91\n",
      "\n",
      "avg / total       0.76      0.76      0.75       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print (cm)\n",
    "print (cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[110  22]\n",
      " [ 31  60]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.83      0.81       132\n",
      "          1       0.73      0.66      0.69        91\n",
      "\n",
      "avg / total       0.76      0.76      0.76       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print (cm)\n",
    "print (cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.83\n",
      "[[112  20]\n",
      " [ 27  64]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.85      0.83       132\n",
      "          1       0.76      0.70      0.73        91\n",
      "\n",
      "avg / total       0.79      0.79      0.79       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "acc_svc_kernel = round(classifier.score(X_train, y_train) * 100, 2)\n",
    "print (acc_svc_kernel)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print (cm)\n",
    "print (cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103  29]\n",
      " [ 25  66]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       132\n",
      "          1       0.69      0.73      0.71        91\n",
      "\n",
      "avg / total       0.76      0.76      0.76       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print (cm)\n",
    "print (cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[109  23]\n",
      " [ 29  62]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.83      0.81       132\n",
      "          1       0.73      0.68      0.70        91\n",
      "\n",
      "avg / total       0.77      0.77      0.77       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print (cm)\n",
    "print (cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[111  21]\n",
      " [ 32  59]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.84      0.81       132\n",
      "          1       0.74      0.65      0.69        91\n",
      "\n",
      "avg / total       0.76      0.76      0.76       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print (cm)\n",
    "print (cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualising the Training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualising the Test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
